# Flexible (A)synchronous Scalable Training

### Performance-oriented flexible distributed framework for deep neural networks data parallel training.

FAST is a framework dedicated to distributed training of deep learning models. it is written in C++11 as a header-only library, but it is designed to be much more flexible.
It can interoperate virtually with any deep learning framework out there, but interfaces built-in are provided for MxNet (C++). More interfaces may come in the future, including python-based training.

This projects is based on GAM (https://github.com/alpha-unito/gam) and FastFlow (https://github.com/fastflow/fastflow) and it is developed by the Alpha parallel computing programming group of university of Turin (http://alpha.di.unito.it/).

Documentation is mainly based on the [examples](./examples), but the [wiki](https://github.com/paoloviviani/FAST/wiki) is being populated.
